{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422d13ff-b4ac-4754-bb63-8a02e3897a92",
   "metadata": {},
   "source": [
    "# TRACE \n",
    "\n",
    "This is a KG-enhanced reader model that employs Knowledge Graphs (KGs) to imporve generation performance: https://arxiv.org/abs/2406.11460.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c7e8410-291c-4db0-8134-841f868106f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-terrier accelerate pyterrier_t5\n",
    "%pip install -q pyterrier_dr \n",
    "%pip install -q pyterrier_caching \n",
    "# %pip install -q pyterrier-rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23965e2-f283-48fc-b41b-e16f69e2ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pyterrier_rag as ptr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10847b72-6863-40ef-8841-acf5be972a20",
   "metadata": {},
   "source": [
    "## Retrieval Setup\n",
    "\n",
    "Lets get a BM25 retriever from PyTerrier. We could also have used a fast Pisa retriever, via [PyTerrier_Pisa](https://github.com/terrierteam/pyterrier_pisa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662cafbc-dcfb-4fb7-aeb3-8fef084bce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:47:07.137 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:47:07.560 [main] WARN org.terrier.structures.BaseCompressingMetaIndex -- Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 1.1 GiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "sparse_index = pt.Artifact.from_hf('pyterrier/raghpq-terrier')\n",
    "bm25 = pt.rewrite.tokenise() >> sparse_index.bm25(include_fields=['docno', 'text', 'title']) >> pt.rewrite.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c259641",
   "metadata": {},
   "source": [
    "# Reader Setup\n",
    "\n",
    "We're going to use a Llama3-8B-Instruct model through Launcher API as the reader model for generating the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.model import get_conversation_template\n",
    "from pyterrier_rag.backend import OpenAIBackend \n",
    "from pyterrier_rag.prompt import PromptTransformer\n",
    "from pyterrier_rag.readers import Reader\n",
    "\n",
    "LAUNCHER_API_KEY = \"put_your_key_here\"  # replace with your key \n",
    "\n",
    "# let us define the prompt \n",
    "system_message = r\"\"\"\n",
    "You are a question answering system that answers based strictly on the provided structured knowledge triples, without using any prior knowledge.\n",
    "\n",
    "You will be given a list of reasoning paths (each is a sequence of knowledge triples), which together support answering the question.\n",
    "\n",
    "Instructions:\n",
    "1. Combine relevant facts across the triples logically.\n",
    "2. If multiple paths are available, prioritize the top-3 scoring ones.\n",
    "3. Provide a concise, factual answer. Do NOT explain your reasoning.\n",
    "4. Do NOT say things like \"Based on the context...\" or \"The chain indicates...\"\n",
    "5. Never hallucinate - only answer what is directly entailed by the reasoning paths.\n",
    "\"\"\"\n",
    "prompt_text = \"\"\"\n",
    "Question: {{ query }}\n",
    "\n",
    "Reasoning Paths:\n",
    "{{ qcontext }}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message,\n",
    "    instruction=prompt_text,\n",
    "    api_type=\"openai\"\n",
    ")\n",
    "\n",
    "# Now we define the LLM backend used in the reader model \n",
    "llm_backend = OpenAIBackend(\n",
    "    model_id=\"llama-3-8b-instruct\", \n",
    "    api_key=LAUNCHER_API_KEY,\n",
    "    generation_args={\"temperature\":0.0, \"max_tokens\":256},\n",
    "    base_url=\"http://api.terrier.org/v1\"\n",
    ")\n",
    "\n",
    "# finally we can obtain a reader \n",
    "reader = Reader(llm_backend, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b3726-0618-4154-9403-baa8cafee837",
   "metadata": {},
   "source": [
    "## TRACE model\n",
    "\n",
    "TRACE model requires converting each retrieved document into a set of knowledge triples and then constructs KG-based reasoning chains from these triples to identify useful information, we will use the `KnowledgeGraphExtractor` and `ReasoningChainGenerator` to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51021048-595f-467d-8062-de63d6aff766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ReasoningChainGenerator with E5.base() ranking model\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_rag import KnowledgeGraphExtractor, ReasoningChainGenerator \n",
    "from pyterrier_dr import E5 \n",
    "from pyterrier_caching import ScorerCache # optional \n",
    "\n",
    "kg_extractor = KnowledgeGraphExtractor(llm_backend) \n",
    "reasoning_chain_generator = ReasoningChainGenerator(\n",
    "    llm_backend, \n",
    "    E5(), \n",
    "    dataset=\"hotpotqa\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "cache_path=\"/nfs/pyterrier/cache\" \n",
    "kg_cache = ScorerCache(cache_path, kg_extractor, group=None, key=\"docno\", value=\"knowledge_graph\", pickle=True) # Cache the extraced KG triples for later use \n",
    "trace = kg_cache >> reasoning_chain_generator >> reader  \n",
    "\n",
    "bm25_trace = (bm25%10) >> trace # TRACE pipeline with BM25 retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b424c47",
   "metadata": {},
   "source": [
    "Lets make a simple RAG pipeline for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc842883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_rag.prompt import Concatenator\n",
    "\n",
    "system_message2 = r\"\"\"You are an expert Q&A system that is trusted around the world.\n",
    "        Always answer the query using the provided context information,\n",
    "        and not prior knowledge.\n",
    "        rules to follow:\n",
    "        1. Not directly reference the given context in your answer\n",
    "        2. Avoid statements like 'Based on the context, ...' or\n",
    "        'The context information ...' or anything along those lines.\"\"\"\n",
    "\n",
    "prompt_text2 = \"\"\"\n",
    "Question: {{ query }}\n",
    "\n",
    "Context information is:{{ qcontext }}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "template = get_conversation_template(\"meta-llama-3.1-sp\")\n",
    "prompt = PromptTransformer(\n",
    "    conversation_template=template,\n",
    "    system_message=system_message2,\n",
    "    instruction=prompt_text2,\n",
    "    api_type=\"openai\"\n",
    ")\n",
    "\n",
    "# >>Concatenator()\n",
    "bm25_reader = Reader(llm_backend, prompt=prompt)\n",
    "bm25 = (bm25%10)>>Concatenator()>>bm25_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037b3a2-570b-4664-848e-3fbf0e420460",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "Now lets run a quick experiment using HotPotQA, comparing vanilla RAG model and TRACE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8abb1807-40a5-4ec6-a43a-f71c1914f2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing results of 50 topics on shared pipeline component (pt.apply.query() >> TerrierRetr(BM25) >> <pyterrier.terrier.rewrite.ResetQuery object at 0x7f77da169bb0>)\n",
      "/root/anaconda3/envs/test/lib/python3.12/site-packages/pyterrier/_evaluation/_execution.py:311: UserWarning: precompute_prefix with batch_size is very experimental. Please report any problems\n",
      "  warn(\"precompute_prefix with batch_size is very experimental. Please report any problems\")\n",
      "pt.Experiment precomputation: 100%|██████████| 2/2 [00:03<00:00,  1.59s/batches]\n",
      "pt.Experiment: 100%|██████████| 4/4 [04:22<00:00, 65.71s/batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    EM        F1  EM +  EM -  EM p-value  F1 +  F1 -  F1 p-value\n",
      "0    RAG  0.02  0.172222   NaN   NaN         NaN   NaN   NaN         NaN\n",
      "1  TRACE  0.16  0.253494   7.0   0.0    0.006833  12.0  11.0    0.157522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = pt.get_dataset('rag:hotpotqa')\n",
    "result = pt.Experiment(\n",
    "    [bm25, bm25_trace],\n",
    "    dataset.get_topics('dev').head(50),\n",
    "    dataset.get_answers('dev'),\n",
    "    [ptr.measures.F1, ptr.measures.EM],\n",
    "    batch_size=25,\n",
    "    verbose=True,\n",
    "    precompute_prefix=True,\n",
    "    names=['RAG', 'TRACE'],\n",
    "    baseline=0\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
